{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Hybrid quantum-classical ML using Keras and PyTorch\n",
    "===================================================\n",
    "\n",
    ".. meta::\n",
    "    :property=\"og:description\": TODO\n",
    "    :property=\"og:image\": TODO\n",
    "\n",
    "In PennyLane, variational quantum circuits are treated as :doc:`quantum nodes\n",
    "<../glossary/hybrid_computation>` (QNodes) that can be thought of simply as functions with a\n",
    "defined output and gradient for a given input. QNodes can be combined with classical nodes to\n",
    "form a hybrid model, which can be trained using the TensorFlow, PyTorch or Autograd/NumPy\n",
    ":doc:`interfaces <introduction/interfaces>`.\n",
    "\n",
    "Composing computational nodes is a fundamental tool of any machine learning package. However,\n",
    "it is often useful to have access to additional high-level functionality for model construction\n",
    "and training. TensorFlow and PyTorch both provide a high-level API for creating neural networks:\n",
    "TensorFlow integrates with `Keras <https://keras.io/>`__ while PyTorch includes a\n",
    "`torch.nn <https://pytorch.org/docs/stable/nn.html>`__ module.\n",
    "\n",
    "This tutorial shows how QNodes in PennyLane can be interfaced with `Keras <https://keras.io/>`__ and\n",
    "`torch.nn <https://pytorch.org/docs/stable/nn.html>`__ to provide a high-level approach to\n",
    "creating hybrid quantum-classical models.\n",
    "\n",
    "Fixing the dataset and problem\n",
    "------------------------------\n",
    "\n",
    "Let us begin by choosing a simple dataset and problem to allow us to focus on how the hybrid\n",
    "model is constructed. Our objective is to classify points generated from scikit-learn's\n",
    "`make_moons() <https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html>`__ dataset:\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import tensorflow as tf\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.1)\n",
    "y_hot = tf.keras.utils.to_categorical(y, num_classes=2)  # one-hot encoded labels\n",
    "\n",
    "c = ['#1f77b4' if y_ == 0 else '#ff7f0e' for y_ in y]\n",
    "plt.axis('off')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=c)\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Defining a QNode\n",
    "# ----------------\n",
    "#\n",
    "# Our next step is to define the QNode that we want to interface with `Keras <https://keras.io/>`__\n",
    "# or `torch.nn <https://pytorch.org/docs/stable/nn.html>`__. Any combination of device, operations\n",
    "# and measurements that is valid in PennyLane can be used to compose the QNode. However,\n",
    "# the QNode arguments must satisfy additional\n",
    "# `conditions <https://pennylane.readthedocs.io/en/stable/code/api/pennylane.qnn.KerasLayer.html#usageDetails>`__\n",
    "# including having an argument called ``inputs``. All other arguments must be arrays or tensors\n",
    "# and are treated as trainable weights in the model. We fix a two-qubit QNode using the\n",
    "# :doc:`default.qubit <code/api/pennylane.plugins.default_qubit.DefaultQubit>` simulator and\n",
    "# operations from the :doc:`templates <introduction/templates>` module.\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "###############################################################################\n",
    "# Note that we do not need to specify the ``interface`` argument in the :func:`~pennylane.qnode`\n",
    "# decorator.\n",
    "#\n",
    "# Interfacing with Keras and Torch\n",
    "# --------------------------------\n",
    "#\n",
    "# With the QNode defined, we are nearly ready to interface with `Keras <https://keras.io/>`__ or\n",
    "# `torch.nn <https://pytorch.org/docs/stable/nn.html>`__. This is achieved using the\n",
    "# :class:`~pennylane.qnn.KerasLayer` and :class:`~pennylane.qnn.TorchLayer` classes of the\n",
    "# :mod:`~pennylane.qnn` module, which convert the QNode to the elementary building block of these\n",
    "# high-level frameworks: a *layer*. We shall see in the following how the resultant layer can be\n",
    "# combined with other well-known neural network layers to form a hybrid model.\n",
    "#\n",
    "# Before doing this, we must first define the ``weight_shapes`` dictionary. Recall that all of\n",
    "# the arguments of the QNode except for the ``inputs``-named argument are treated as trainable\n",
    "# weights. For the QNode to be successfully converted to a layer in `Keras <https://keras.io/>`__ or\n",
    "# `torch.nn <https://pytorch.org/docs/stable/nn.html>`__, we need to provide the details of the\n",
    "# shape of each trainable weight for them to be initialized. The ``weight_shapes`` dictionary\n",
    "# maps from the argument names of the QNode to corresponding weights:\n",
    "\n",
    "n_layers = 3\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "\n",
    "###############################################################################\n",
    "# In our example, the ``weights`` argument of the QNode is trainable and has shape given by\n",
    "# ``(n_layers, n_qubits, 3)`` which are passed to\n",
    "# :func:`~pennylane.templates.layers.StronglyEntanglingLayers`.\n",
    "#\n",
    "# With ``weight_shapes`` defined, it is easy to then convert the QNode. To convert to\n",
    "# `Keras <https://keras.io/>`__:\n",
    "\n",
    "qlayer_tf = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n",
    "\n",
    "###############################################################################\n",
    "# To convert to `torch.nn <https://pytorch.org/docs/stable/nn.html>`__:\n",
    "\n",
    "qlayer_torch = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "###############################################################################\n",
    "# With this done, the rest is just working with your favourite classical machine learning library!\n",
    "#\n",
    "# Creating a hybrid model\n",
    "# -----------------------\n",
    "#\n",
    "# Let's create a basic three-layered hybrid model consisting of:\n",
    "#\n",
    "# 1. A 2-neuron fully connected classical layer\n",
    "# 2. Our 2-qubit QNode converted into a layer\n",
    "# 3. Another 2-neuron fully connected classical layer\n",
    "# 4. A softmax activation to convert to a probability vector\n",
    "#\n",
    "# This can be done using the ``Sequential`` API in both `Keras <https://keras.io/>`__ and `torch.nn\n",
    "# <https://pytorch.org/docs/stable/nn.html>`__. First, using the `Keras <https://keras.io/>`__\n",
    "# `Sequential <https://www.tensorflow.org/api_docs/python/tf/keras/Sequential>`__:\n",
    "\n",
    "clayer1_tf = tf.keras.layers.Dense(2)\n",
    "clayer2_tf = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "model_tf = tf.keras.models.Sequential([clayer1_tf, qlayer_tf, clayer2_tf])\n",
    "\n",
    "###############################################################################\n",
    "# Similarly, using `torch.nn <https://pytorch.org/docs/stable/nn.html>`__\n",
    "# `Sequential <https://pytorch.org/docs/stable/nn.html#sequential>`__:\n",
    "\n",
    "import torch\n",
    "\n",
    "clayer1_torch = torch.nn.Linear(2, 2)\n",
    "clayer2_torch = torch.nn.Linear(2, 2)\n",
    "softmax_torch = torch.nn.Softmax(dim=1)\n",
    "model_torch = torch.nn.Sequential(clayer1_torch, qlayer_torch, clayer2_torch, softmax_torch)\n",
    "\n",
    "###############################################################################\n",
    "# Constructing hybrid models is easy!\n",
    "#\n",
    "# Training the model\n",
    "# ------------------\n",
    "#\n",
    "# We can now train our hybrid model on the the classification dataset using the usual\n",
    "# approaches in `Keras <https://keras.io/>`__ and\n",
    "# `torch.nn <https://pytorch.org/docs/stable/nn.html>`__. Let's focus on Keras. We'll use the\n",
    "# standard `SGD <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD>`__ optimizer\n",
    "# and the mean absolute error loss function:\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.2)\n",
    "model_tf.compile(opt, loss='mae', metrics=['accuracy'])\n",
    "\n",
    "###############################################################################\n",
    "# Note that there are more advanced combinations of optimizer and loss function, but here we are\n",
    "# focusing on the basics.\n",
    "#\n",
    "# The model is now ready to be trained!\n",
    "\n",
    "X = X.astype(\"float32\")\n",
    "y = y.astype(\"float32\")\n",
    "model_tf.fit(X, y, epochs=8, batch_size=5, validation_split=0.25)\n",
    "\n",
    "###############################################################################\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlayer_tf_2 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n",
    "clayer1_tf = tf.keras.layers.Dense(4)\n",
    "clayer2_tf = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,))\n",
    "x = clayer1_tf(inputs)\n",
    "x_1, x_2 = tf.split(x, 2, axis=1)\n",
    "x_1 = qlayer_tf(x_1)\n",
    "x_2 = qlayer_tf_2(x_2)\n",
    "x = tf.concat([x_1, x_2], axis=1)\n",
    "outputs = clayer2_tf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.2)\n",
    "model.compile(opt, loss='mae', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.5000 - accuracy: 0.5000 - val_loss: 0.5000 - val_accuracy: 0.5000\n",
      "Epoch 2/8\n",
      "30/30 [==============================] - 14s 474ms/step - loss: 0.5000 - accuracy: 0.5000 - val_loss: 0.5000 - val_accuracy: 0.5000\n",
      "Epoch 3/8\n",
      " 5/30 [====>.........................] - ETA: 9s - loss: 0.5000 - accuracy: 0.5600"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=8, batch_size=5, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
